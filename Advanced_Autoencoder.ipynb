{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Noisy data set generation.\n",
    "\n",
    "Author: Natalie Klein\n",
    "\n",
    "TODO: Add other noise generation models.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from splitml.signal import VoigtSignal\n",
    "\n",
    "def sig_gen(t, df):\n",
    "    \"\"\"\n",
    "    Generate collection of Voigt time series signals based on Pandas data frame of parameter values.\n",
    "    Args:\n",
    "        t: vector of time points\n",
    "        df: Pandas data frame containing signal parameter columns A, w, T2, phi, sigma, C, with N rows\n",
    "    Returns:\n",
    "        np.ndarray of signals, shape (N, len(t))\n",
    "    \"\"\"\n",
    "    sigs = np.zeros((df.shape[0], len(t)), dtype=complex)\n",
    "    for i in range(df.shape[0]):\n",
    "        sigs[i, :] = VoigtSignal(df.w.iloc[i], df.T2.iloc[i], df.A.iloc[i], df.phi.iloc[i], df.sigma.iloc[i], df.C.iloc[i]).time_signal(t)\n",
    "    return sigs\n",
    "    \n",
    "def wn_gen(t, N, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Generate complex Gaussian white noise time series of shape (N, t) with total variance sigma.\n",
    "    \"\"\"\n",
    "    return np.random.normal(0, scale=sigma/np.sqrt(2), size=(N, len(t))) + 1j* np.random.normal(0, scale=sigma/np.sqrt(2), size=(N, len(t)))\n",
    "\n",
    "def split_noise(noise, n_timesteps):\n",
    "    \"\"\"\n",
    "    Split noise data array along time axis to increase number of examples (with shorter duration).\n",
    "    \"\"\"\n",
    "    nt = noise.shape[1]\n",
    "    nseg = nt // n_timesteps\n",
    "    noise = noise[:, :(nseg*n_timesteps)]\n",
    "    snoise = np.stack(np.hsplit(noise, nseg), axis=1)\n",
    "    rsnoise = np.reshape(snoise, (noise.shape[0]*nseg, -1))\n",
    "    return rsnoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom Pytorch layers for complex-valued tensors.\n",
    "\n",
    "\n",
    "ComplexiSTFT based on code from torchaudio.\n",
    "\n",
    "BSD 2-Clause License\n",
    "\n",
    "Copyright (c) 2017 Facebook Inc. (Soumith Chintala), \n",
    "Copyright (c) 2022, Triad National Security, LLC\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "* Redistributions of source code must retain the above copyright notice, this\n",
    "  list of conditions and the following disclaimer.\n",
    "\n",
    "* Redistributions in binary form must reproduce the above copyright notice,\n",
    "  this list of conditions and the following disclaimer in the documentation\n",
    "  and/or other materials provided with the distribution.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, Conv1d, ConvTranspose1d\n",
    "\n",
    "class ComplexConv1d(Module):\n",
    "    \"\"\" Pytorch Conv1d adapted to complex-valued\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,in_channels, out_channels, kernel_size=3, stride=1, padding = 0,\n",
    "                 dilation=1, groups=1, bias=True):\n",
    "        super(ComplexConv1d, self).__init__()\n",
    "        self.conv_r = Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        self.conv_i = Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        \n",
    "    def forward(self, input):    \n",
    "        fwd_rr = self.conv_r(input.real)\n",
    "        fwd_ri = self.conv_r(input.imag)\n",
    "        fwd_ir = self.conv_i(input.real)\n",
    "        fwd_ii = self.conv_i(input.imag)\n",
    "        output = torch.zeros_like(fwd_rr.type(input.dtype))\n",
    "        output.real = fwd_rr - fwd_ii\n",
    "        output.imag = fwd_ri + fwd_ir\n",
    "        return output\n",
    "\n",
    "class ComplexConvTranspose1d(Module):\n",
    "    \"\"\" Pytorch ConvTranspose1d adapted to complex-valued\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                 output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros'):\n",
    "\n",
    "        super(ComplexConvTranspose1d, self).__init__()\n",
    "\n",
    "        self.conv_tran_r = ConvTranspose1d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                                       output_padding, groups, bias, dilation, padding_mode)\n",
    "        self.conv_tran_i = ConvTranspose1d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                                       output_padding, groups, bias, dilation, padding_mode)\n",
    "\n",
    "    def forward(self,input):\n",
    "        fwd_rr = self.conv_tran_r(input.real)\n",
    "        fwd_ri = self.conv_tran_r(input.imag)\n",
    "        fwd_ir = self.conv_tran_i(input.real)\n",
    "        fwd_ii = self.conv_tran_i(input.imag)\n",
    "        output = torch.zeros_like(fwd_rr.type(input.dtype))\n",
    "        output.real = fwd_rr - fwd_ii\n",
    "        output.imag = fwd_ri + fwd_ir\n",
    "        return output\n",
    "\n",
    "class ComplexiSTFT(Module):\n",
    "    \"\"\" torchaudio.functional.inverse_spectrogram, adapted to give complex-valued output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_fft, hop_length, win_length, pad=0):\n",
    "        super(ComplexiSTFT, self).__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.pad = pad\n",
    "        self.window = torch.hann_window(self.win_length)\n",
    "        self.center = True\n",
    "        self.length = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        shape = input.size()\n",
    "        spectrogram = input.reshape(-1, shape[-2], shape[-1])\n",
    "        waveform = torch.istft(\n",
    "            input=spectrogram,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=self.window.to(input.get_device()),\n",
    "            center=self.center,\n",
    "            normalized=False,\n",
    "            onesided=False,\n",
    "            length=self.length + 2 * self.pad if self.length is not None else None,\n",
    "            return_complex=True\n",
    "        )\n",
    "        if self.length is not None and self.pad > 0:\n",
    "            # remove padding from front and back\n",
    "            waveform = waveform[:, self.pad:-self.pad]\n",
    "\n",
    "        # unpack batch\n",
    "        waveform = waveform.reshape(shape[:-2] + waveform.shape[-1:])\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Generation__\n",
    "\n",
    "Generated and saved clean and noisy data separately for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#### Initialize Parameters ###\n",
    "t = np.arange(0, 20, 0.1) # equally spaced time points\n",
    "nrow = 100 # observations of sine waves\n",
    "ncol = len(t) # time points\n",
    "per_Lbound = 2 # lower bound for period\n",
    "per_Ubound = 5 # upper bound for period\n",
    "mu = 0 # noise parameter\n",
    "sig_sq = 0.5 # noise parameter\n",
    "A = 1 # initial amplitude \n",
    "d = 0.2 # amplitude decay constant\n",
    "amp = np.exp(-d*t)*A # amplitude\n",
    "\n",
    "### Generate Training Data ###\n",
    "w = np.random.randint(per_Lbound,per_Ubound, nrow) # random period of the sine wave\n",
    "p = np.random.uniform(0,2*math.pi, nrow) # random starting point in the cycle\n",
    "noise = np.random.normal(mu,sig_sq,(nrow,ncol)) # epsilon follows N(mu, sig_sq)\n",
    "\n",
    "y_clean_training = np.zeros((nrow,ncol)) \n",
    "y_noisy_training = np.zeros((nrow,ncol))\n",
    "for i in range(nrow): \n",
    "    y_clean_training[i,:] = (np.sin(t*w[i]+p[i]))*amp # y = np.sin(t*w+p) \n",
    "    y_noisy_training[i,:] = y_clean_training[i,:] + noise[i,:] # y = np.sin(t*w+p) + Ïµ\n",
    "\n",
    "### Generate Validation Data ###\n",
    "w = np.random.randint(per_Lbound,per_Ubound, nrow)\n",
    "p = np.random.uniform(0,2*math.pi, nrow) \n",
    "noise = np.random.normal(mu,sig_sq,(nrow,ncol))\n",
    "\n",
    "y_clean_validation = np.zeros((nrow,ncol)) \n",
    "y_noisy_validation = np.zeros((nrow,ncol))\n",
    "for i in range(nrow): \n",
    "    y_clean_validation[i,:] = (np.sin(t*w[i]+p[i]))*amp \n",
    "    y_noisy_validation[i,:] = y_clean_validation[i,:] + noise[i,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compare Noisy and Clean Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_clean_training[1,:]) # clean data\n",
    "plt.plot(y_noisy_training[1,:]) # noisy data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Autoencoder__\n",
    "\n",
    "Currently has two linear layers of input size $M$ and $H$ respectively with `hardtanh` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "### Neural Network ###\n",
    "class simpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, t_input = len(t), M=10, H=5): \n",
    "        super(simpleNet, self).__init__()\n",
    "        self.transform_lin_layer = nn.Linear(t_input, M) \n",
    "        self.transform_hidden_layer = nn.Linear(M, H)\n",
    "        self.inverse_hidden_layer = nn.Linear(H, M)\n",
    "        self.inverse_lin_layer = nn.Linear(M, t_input)\n",
    "        \n",
    "        self.activation = F.hardtanh\n",
    "        \n",
    "    def forward(self, t): \n",
    "        m = self.activation(self.transform_lin_layer(t))\n",
    "        h = self.activation(self.transform_hidden_layer(m))\n",
    "        m = self.activation(self.inverse_hidden_layer(h))\n",
    "        t = self.inverse_lin_layer(m)\n",
    "        \n",
    "        return t\n",
    "\n",
    "    def embed(self, t): \n",
    "        m = self.activation(self.transform_lin_layer(t))\n",
    "        h = self.activation(self.transform_hidden_layer(m))\n",
    "        \n",
    "        return h\n",
    "\n",
    "net = simpleNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing for training ###\n",
    "device = torch.device('cpu')\n",
    "model = simpleNet().to(device) \n",
    "\n",
    "noisy_training_data = torch.tensor(y_noisy_training).float().to(device)\n",
    "clean_training_data = torch.tensor(y_clean_training).float().to(device)\n",
    "\n",
    "noisy_validation_data = torch.tensor(y_noisy_validation).float().to(device)\n",
    "clean_validation_data = torch.tensor(y_clean_validation).float().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train() \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "use_clean_training_data = False\n",
    "use_clean_validation_data = False\n",
    "patience = 5\n",
    "trigger_times = 0\n",
    "loss_training = []\n",
    "loss_validation = []\n",
    "\n",
    "\n",
    "### Training ###\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad() \n",
    "    out = model(noisy_training_data)\n",
    "    if use_clean_training_data is False:\n",
    "        loss = criterion(out, noisy_training_data)\n",
    "    else:\n",
    "        loss = criterion(out, clean_training_data)\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    loss_training.append(loss.item())\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss.item())\n",
    "    with torch.no_grad(): \n",
    "        out = model(noisy_validation_data)\n",
    "        if use_clean_validation_data is False:\n",
    "            loss = criterion(out, noisy_validation_data)\n",
    "        else: \n",
    "            loss = criterion(out, clean_validation_data)\n",
    "        loss_validation.append(loss.item())\n",
    "    if epoch > 200:\n",
    "        if loss_training[-1] > loss_training[-2]:\n",
    "            trigger_times += 1\n",
    "            \n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStopped at epoch')\n",
    "                print(epoch)\n",
    "                break\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot of Loss__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(loss_training)\n",
    "plt.plot(loss_validation)\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "begin = round(0.3*len(loss_training))\n",
    "end = len(loss_training)\n",
    "plt.plot(loss_training[begin:end])\n",
    "plt.plot(loss_validation[begin:end])\n",
    "plt.title(\"Loss Zoomed In\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot of Original and Reproduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(noisy_validation_data)\n",
    "row = 1\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(t, y_clean_validation[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Validation\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(t, y_noisy_validation[row,:])\n",
    "plt.plot(t, y_clean_validation[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Validation with Noise\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "out = model(noisy_training_data)\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t, y_clean_training[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Training\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(t, y_noisy_training[row,:])\n",
    "plt.plot(t, y_clean_training[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Training with Noise\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(noisy_validation_data)\n",
    "row = 2\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t, y_clean_validation[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Validation\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(t, y_noisy_validation[row,:])\n",
    "plt.plot(t, y_clean_validation[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Validation with Noise\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "out = model(noisy_training_data)\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(t, y_clean_training[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Training\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(t, y_noisy_training[row,:])\n",
    "plt.plot(t, y_clean_training[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Training with Noise\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation 3__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(noisy_validation_data)\n",
    "row = 3\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t, y_clean_validation[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Validation\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(t, y_noisy_validation[row,:])\n",
    "plt.plot(t, y_clean_validation[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Validation with Noise\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "out = model(noisy_training_data)\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(t, y_clean_training[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Training\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(t, y_noisy_training[row,:])\n",
    "plt.plot(t, y_clean_training[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Training with Noise\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation 4__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(noisy_validation_data)\n",
    "row = 4\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t, y_clean_validation[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Validation\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(t, y_noisy_validation[row,:])\n",
    "plt.plot(t, y_clean_validation[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Validation with Noise\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "out = model(noisy_training_data)\n",
    "\n",
    "fig = plt.figure() \n",
    "fig.set_size_inches(14,6) \n",
    "\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(t, y_clean_training[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Training\")\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(t, y_noisy_training[row,:])\n",
    "plt.plot(t, y_clean_training[row,:])\n",
    "plt.plot(t, out.detach().numpy()[row,:])\n",
    "plt.title(\"Training with Noise\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('pytorch_env_clone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23cb4257b5714976b9713a5051b2290373f560914ecc83796a64c386f1580576"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
